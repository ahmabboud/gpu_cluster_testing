apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: gpu-cluster-test-multi-gpu-nebius
  namespace: default
spec:
  runPolicy:
    cleanPodPolicy: Running  # Keep completed pods for log inspection
    ttlSecondsAfterFinished: 86400  # Delete job after 24 hours

  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: gpu-cluster-test
            role: master
        spec:
          containers:
          - name: pytorch
            image: ghcr.io/ahmabboud/gpu_cluster_testing:latest
            imagePullPolicy: Always
            
            # ====================================
            # NEBIUS BEST PRACTICE: Multi-GPU Resources
            # Allocate ALL GPUs on a node (8 for H100)
            # CPU: ~14 per GPU = 112 total
            # Memory: ~150GB per GPU = 1200GB total
            # ====================================
            resources:
              requests:
                nvidia.com/gpu: 8
                cpu: "32"
                memory: "128Gi"
              limits:
                nvidia.com/gpu: 8
                cpu: "64"
                memory: "256Gi"
            
            # ====================================
            # NEBIUS BEST PRACTICE: Shared Memory
            # Required for PyTorch DataLoader workers
            # Prevents "No space left on device" errors
            # ====================================
            volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            
            env:
            - name: NCCL_DEBUG
              value: "INFO"  # WARN for production, INFO for debugging
            - name: NCCL_SOCKET_IFNAME
              value: "eth0"  # Change to "ib0" if using InfiniBand
            # Uncomment for InfiniBand clusters:
            # - name: NCCL_IB_HCA
            #   value: "mlx5"
            # - name: NCCL_IB_DISABLE
            #   value: "0"
            # - name: UCX_NET_DEVICES
            #   value: "mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1"
            
            # PyTorch distributed settings
            - name: LOGLEVEL
              value: "INFO"
            
            command:
            - "python"
            - "/workspace/src/train.py"
            args:
            - "--model"
            - "resnet50"
            - "--batch-size"
            - "64"
            - "--warmup-iterations"
            - "20"
            - "--active-iterations"
            - "100"
            - "--data-mode"
            - "synthetic"
          
          # ====================================
          # NEBIUS BEST PRACTICE: Shared Memory Volume
          # ====================================
          volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 32Gi  # Adjust based on needs
    
    Worker:
      replicas: 1  # Number of worker nodes (each with 8 GPUs)
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: gpu-cluster-test
            role: worker
        spec:
          # Disable service account token mounting (security best practice)
          automountServiceAccountToken: false
          
          containers:
          - name: pytorch
            image: ghcr.io/ahmabboud/gpu_cluster_testing:latest
            imagePullPolicy: Always
            
            # ====================================
            # NEBIUS BEST PRACTICE: Multi-GPU Resources
            # ====================================
            resources:
              requests:
                nvidia.com/gpu: 8
                cpu: "32"
                memory: "128Gi"
              limits:
                nvidia.com/gpu: 8
                cpu: "64"
                memory: "256Gi"
            
            # ====================================
            # NEBIUS BEST PRACTICE: Shared Memory
            # ====================================
            volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            
            # Environment variables (same as master)
            env:
            - name: NCCL_DEBUG
              value: "INFO"
            - name: NCCL_SOCKET_IFNAME
              value: "eth0"
            # Uncomment for InfiniBand:
            # - name: NCCL_IB_HCA
            #   value: "mlx5"
            # - name: NCCL_IB_DISABLE
            #   value: "0"
            # - name: UCX_NET_DEVICES
            #   value: "mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1"
            
            command:
            - "python"
            - "/workspace/src/train.py"
            args:
            - "--model"
            - "resnet50"
            - "--batch-size"
            - "64"
            - "--warmup-iterations"
            - "20"
            - "--active-iterations"
            - "100"
            - "--data-mode"
            - "synthetic"
          
          # ====================================
          # NEBIUS BEST PRACTICE: Shared Memory Volume
          # ====================================
          volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 32Gi
